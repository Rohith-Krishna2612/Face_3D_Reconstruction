{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7df5cb4",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21a6f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify GPU availability\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "print(f\"\\n‚úÖ PyTorch version: {torch.__version__}\")\n",
    "print(f\"‚úÖ CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"‚úÖ VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7983d90f",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91eecca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create project directory in Drive\n",
    "import os\n",
    "project_dir = '/content/drive/MyDrive/Face_3D_Reconstruction'\n",
    "os.makedirs(project_dir, exist_ok=True)\n",
    "print(f\"‚úÖ Project directory: {project_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29b9cdb",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Clone Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b66e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone your GitHub repo\n",
    "%cd /content\n",
    "!git clone https://github.com/Rohith-Krishna2612/Face_3D_Reconstruction.git\n",
    "%cd Face_3D_Reconstruction\n",
    "\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0f7c72",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3514b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PyTorch with CUDA (Colab comes with it, but ensure correct version)\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 -q\n",
    "\n",
    "# Install other dependencies\n",
    "!pip install -r requirements.txt -q\n",
    "\n",
    "print(\"\\n‚úÖ Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3417c337",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Setup FFHQ Dataset\n",
    "\n",
    "**Option A: Download from Kaggle** (Recommended - one-time setup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5aeda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Kaggle API\n",
    "# 1. Go to https://www.kaggle.com/settings/account\n",
    "# 2. Click \"Create New API Token\" to download kaggle.json\n",
    "# 3. Upload kaggle.json using the file upload button below\n",
    "\n",
    "from google.colab import files\n",
    "print(\"üì§ Upload your kaggle.json file:\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Setup Kaggle credentials\n",
    "!mkdir -p ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "\n",
    "print(\"\\n‚úÖ Kaggle API configured!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c142320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download FFHQ dataset from Kaggle (only need to do once)\n",
    "import os\n",
    "\n",
    "# Check if dataset already exists in Drive\n",
    "drive_dataset_path = '/content/drive/MyDrive/Face_3D_Reconstruction/data/ffhq'\n",
    "\n",
    "if os.path.exists(drive_dataset_path) and len(os.listdir(drive_dataset_path)) > 1000:\n",
    "    print(f\"‚úÖ Dataset already exists in Drive: {drive_dataset_path}\")\n",
    "    print(f\"‚úÖ Image count: {len(os.listdir(drive_dataset_path))}\")\n",
    "else:\n",
    "    print(\"‚¨áÔ∏è Downloading FFHQ dataset from Kaggle...\")\n",
    "    print(\"‚ö†Ô∏è This will take 10-15 minutes (13GB download)\\n\")\n",
    "    \n",
    "    # Download using kagglehub\n",
    "    !pip install kagglehub -q\n",
    "    \n",
    "    import kagglehub\n",
    "    path = kagglehub.dataset_download(\"arnaud58/flickrfaceshq-dataset-ffhq\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Downloaded to: {path}\")\n",
    "    \n",
    "    # Move to Drive for persistence\n",
    "    print(\"\\nüìÅ Moving to Google Drive for persistence...\")\n",
    "    !mkdir -p {drive_dataset_path}\n",
    "    !cp -r {path}/* {drive_dataset_path}/\n",
    "    \n",
    "    print(f\"\\n‚úÖ Dataset saved to Drive: {drive_dataset_path}\")\n",
    "    print(f\"‚úÖ You won't need to download again!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eea319d",
   "metadata": {},
   "source": [
    "**Option B: Upload from Local** (If you already downloaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3058dff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip this if you used Option A above\n",
    "# Manually upload FFHQ dataset to Google Drive at:\n",
    "# /content/drive/MyDrive/Face_3D_Reconstruction/data/ffhq/\n",
    "\n",
    "# Verify dataset\n",
    "import os\n",
    "drive_dataset_path = '/content/drive/MyDrive/Face_3D_Reconstruction/data/ffhq'\n",
    "if os.path.exists(drive_dataset_path):\n",
    "    count = len([f for f in os.listdir(drive_dataset_path) if f.endswith('.png')])\n",
    "    print(f\"‚úÖ Found {count} images in Drive\")\n",
    "else:\n",
    "    print(\"‚ùå Dataset not found. Please use Option A or upload manually.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ca21cf",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Create Symlink to Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f24ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create symlink from Drive to local data folder (faster access)\n",
    "import os\n",
    "\n",
    "local_data_dir = '/content/Face_3D_Reconstruction/data'\n",
    "drive_dataset_path = '/content/drive/MyDrive/Face_3D_Reconstruction/data/ffhq'\n",
    "\n",
    "os.makedirs(local_data_dir, exist_ok=True)\n",
    "\n",
    "# Remove existing symlink if present\n",
    "if os.path.exists(f\"{local_data_dir}/ffhq\"):\n",
    "    !rm -rf {local_data_dir}/ffhq\n",
    "\n",
    "# Create symlink\n",
    "!ln -s {drive_dataset_path} {local_data_dir}/ffhq\n",
    "\n",
    "# Verify\n",
    "count = len([f for f in os.listdir(f\"{local_data_dir}/ffhq\") if f.endswith('.png')])\n",
    "print(f\"‚úÖ Symlink created: {local_data_dir}/ffhq\")\n",
    "print(f\"‚úÖ Accessible images: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230855c8",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Update Config for Colab (512√ó512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e47ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update config.yaml for Colab T4 GPU\n",
    "import yaml\n",
    "\n",
    "with open('config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Optimize for T4 GPU (16GB VRAM)\n",
    "config['dataset']['resolution'] = 512  # Higher resolution\n",
    "config['training']['batch_size'] = 4  # Bigger batch\n",
    "config['training']['num_workers'] = 2\n",
    "config['training']['gradient_accumulation_steps'] = 2  # Less needed with batch=4\n",
    "\n",
    "# Save checkpoints to Drive\n",
    "config['paths']['checkpoints'] = '/content/drive/MyDrive/Face_3D_Reconstruction/checkpoints'\n",
    "config['paths']['logs'] = '/content/drive/MyDrive/Face_3D_Reconstruction/logs'\n",
    "config['paths']['output'] = '/content/drive/MyDrive/Face_3D_Reconstruction/output'\n",
    "\n",
    "# Save updated config\n",
    "with open('config.yaml', 'w') as f:\n",
    "    yaml.dump(config, f)\n",
    "\n",
    "print(\"‚úÖ Config updated for Colab T4:\")\n",
    "print(f\"   Resolution: {config['dataset']['resolution']}√ó{config['dataset']['resolution']}\")\n",
    "print(f\"   Batch size: {config['training']['batch_size']}\")\n",
    "print(f\"   Gradient accumulation: {config['training']['gradient_accumulation_steps']}\")\n",
    "print(f\"   Effective batch: {config['training']['batch_size'] * config['training']['gradient_accumulation_steps']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56cb17c",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Quick Test (Optional - 15 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c134db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test with 1000 samples to verify everything works\n",
    "!python quick_train.py --epochs 2 --max-samples 1000\n",
    "\n",
    "print(\"\\n‚úÖ Quick test complete! If this worked, proceed to full training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b90549",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Start Full Training (512√ó512)\n",
    "\n",
    "**‚ö†Ô∏è Important Notes:**\n",
    "- Training will take ~5-7 hours\n",
    "- Colab Free has 12-hour limit (should be enough)\n",
    "- Don't close browser tab\n",
    "- Checkpoints auto-save to Google Drive every 5 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ba54d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Train with 5000 samples (recommended - ~5 hours)\n",
    "!python quick_train.py --epochs 20 --max-samples 5000\n",
    "\n",
    "# Option 2: Full dataset training (if you have Colab Pro - ~12 hours)\n",
    "# !python train.py\n",
    "\n",
    "print(\"\\nüéâ Training complete!\")\n",
    "print(\"‚úÖ Best model saved to Google Drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087ff146",
   "metadata": {},
   "source": [
    "## üîü Monitor Training (Run in parallel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33229fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start TensorBoard\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir /content/drive/MyDrive/Face_3D_Reconstruction/logs/codeformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dc7dfa",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£1Ô∏è‚É£ Check Sample Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aeb817d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View sample restoration outputs\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "output_dir = '/content/drive/MyDrive/Face_3D_Reconstruction/output/samples'\n",
    "samples = sorted(os.listdir(output_dir))[-5:]  # Last 5 samples\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(20, 4))\n",
    "for idx, sample in enumerate(samples):\n",
    "    img = Image.open(os.path.join(output_dir, sample))\n",
    "    axes[idx].imshow(img)\n",
    "    axes[idx].axis('off')\n",
    "    axes[idx].set_title(sample)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ Showing last 5 sample outputs from: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5f95ae",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£2Ô∏è‚É£ Download Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be94f928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the best trained model to your local machine\n",
    "from google.colab import files\n",
    "\n",
    "checkpoint_dir = '/content/drive/MyDrive/Face_3D_Reconstruction/checkpoints/codeformer'\n",
    "best_model = os.path.join(checkpoint_dir, 'best_checkpoint.pth')\n",
    "\n",
    "if os.path.exists(best_model):\n",
    "    print(f\"üì• Downloading best model: {best_model}\")\n",
    "    files.download(best_model)\n",
    "    print(\"\\n‚úÖ Download complete!\")\n",
    "    print(\"\\nüìÇ Place it in your local: checkpoints/codeformer/\")\n",
    "else:\n",
    "    print(f\"‚ùå Best model not found at: {best_model}\")\n",
    "    print(\"\\nAvailable checkpoints:\")\n",
    "    !ls -lh {checkpoint_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba57717f",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£3Ô∏è‚É£ Training Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa661cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display final training metrics\n",
    "import json\n",
    "\n",
    "metrics_file = '/content/drive/MyDrive/Face_3D_Reconstruction/logs/codeformer/metrics.json'\n",
    "\n",
    "if os.path.exists(metrics_file):\n",
    "    with open(metrics_file, 'r') as f:\n",
    "        metrics = json.load(f)\n",
    "    \n",
    "    print(\"üìä Final Training Metrics:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Best Epoch: {metrics.get('best_epoch', 'N/A')}\")\n",
    "    print(f\"Best PSNR: {metrics.get('best_psnr', 'N/A'):.2f} dB\")\n",
    "    print(f\"Best SSIM: {metrics.get('best_ssim', 'N/A'):.4f}\")\n",
    "    print(f\"Final Loss: {metrics.get('final_loss', 'N/A'):.4f}\")\n",
    "    print(f\"Training Time: {metrics.get('total_time', 'N/A')}\")\n",
    "    print(\"=\" * 50)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Metrics file not found. Check logs directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e113d4",
   "metadata": {},
   "source": [
    "## üìù Summary\n",
    "\n",
    "### What You Just Did:\n",
    "‚úÖ Trained CodeFormer at **512√ó512** resolution (better than local 256√ó256)  \n",
    "‚úÖ Used T4 GPU with 16GB VRAM (4x more than RTX 3050)  \n",
    "‚úÖ Batch size 4 (2x larger than local)  \n",
    "‚úÖ Training time: ~5-7 hours (similar to local but better quality)  \n",
    "‚úÖ All outputs saved to Google Drive  \n",
    "\n",
    "### Next Steps:\n",
    "1. Download `best_checkpoint.pth` from cell above\n",
    "2. Place in your local: `checkpoints/codeformer/`\n",
    "3. Test with web interface: `cd backend && python main.py`\n",
    "4. Deploy Phase 1 complete! üéâ\n",
    "\n",
    "### Files in Your Google Drive:\n",
    "```\n",
    "/MyDrive/Face_3D_Reconstruction/\n",
    "‚îú‚îÄ‚îÄ checkpoints/codeformer/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ best_checkpoint.pth (download this!)\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ epoch_5.pth\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ epoch_10.pth\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
    "‚îú‚îÄ‚îÄ logs/codeformer/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ (TensorBoard logs)\n",
    "‚îú‚îÄ‚îÄ output/samples/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ (sample images)\n",
    "‚îî‚îÄ‚îÄ data/ffhq/\n",
    "    ‚îî‚îÄ‚îÄ (70K images - keep for future use)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
